{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elenasofia98/PracticalNLP-2023-2024/blob/main/HoL07_3_XAI_Kermitz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIGLp3IjJP8g"
      },
      "source": [
        "# KERMIT and KERMITviz üê∏\n",
        "\n",
        "KERMIT paper: https://virtual.2020.emnlp.org/paper_main.1210.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSvhI4cuoiI3",
        "outputId": "6d05f2dd-298e-4abd-d641-16b9407c917e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KERMIT'...\n",
            "remote: Enumerating objects: 653, done.\u001b[K\n",
            "remote: Counting objects: 100% (234/234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 653 (delta 163), reused 62 (delta 41), pack-reused 419\u001b[K\n",
            "Receiving objects: 100% (653/653), 14.66 MiB | 17.23 MiB/s, done.\n",
            "Resolving deltas: 100% (335/335), done.\n"
          ]
        }
      ],
      "source": [
        "#Clone repository and install libraries\n",
        "!rm -r KERMIT\n",
        "!git clone https://github.com/ART-Group-it/KERMIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSXvXlhM8W3N",
        "outputId": "316ef5fe-4ad4-44d4-bd7e-19db2a3b8757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'shared_weights' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ariana2011/shared_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cht7zfwl3JhT",
        "outputId": "287487f7-a5f6-4c9b-f4bf-33140f84ef4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTk6jZG06Ww8",
        "outputId": "79b3bcc7-9d78-4db1-fb12-274bc1a48be1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H0dYDTE-fqT",
        "outputId": "f248d0fb-f9fe-442e-cfe9-72a7307b89b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./KERMIT/kerMIT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kerMIT==2.0) (1.23.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from kerMIT==2.0) (3.8.1)\n",
            "Requirement already satisfied: colormap in /usr/local/lib/python3.10/dist-packages (from kerMIT==2.0) (1.0.6)\n",
            "Requirement already satisfied: easydev in /usr/local/lib/python3.10/dist-packages (from kerMIT==2.0) (0.12.1)\n",
            "Requirement already satisfied: StanfordCoreNLP in /usr/local/lib/python3.10/dist-packages (from kerMIT==2.0) (3.9.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from kerMIT==2.0) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from colormap->kerMIT==2.0) (3.7.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from easydev->kerMIT==2.0) (0.4.6)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.10/dist-packages (from easydev->kerMIT==2.0) (4.9.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from easydev->kerMIT==2.0) (6.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->kerMIT==2.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->kerMIT==2.0) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->kerMIT==2.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->kerMIT==2.0) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->kerMIT==2.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->kerMIT==2.0) (2023.3.post1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from StanfordCoreNLP->kerMIT==2.0) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from StanfordCoreNLP->kerMIT==2.0) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->kerMIT==2.0) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->colormap->kerMIT==2.0) (3.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect->easydev->kerMIT==2.0) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->StanfordCoreNLP->kerMIT==2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->StanfordCoreNLP->kerMIT==2.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->StanfordCoreNLP->kerMIT==2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->StanfordCoreNLP->kerMIT==2.0) (2023.11.17)\n",
            "Building wheels for collected packages: kerMIT\n",
            "  Building wheel for kerMIT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerMIT: filename=kerMIT-2.0-py3-none-any.whl size=203665 sha256=9f05902fa48d8ef5cbd6e166b76d8f9ce7f6cea94216d153f2c7c446619fa6de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d_v7cm3q/wheels/b5/83/6e/92611dbf1d7b0ec1799f68368b47833243d7bfc31560b5d4cb\n",
            "Successfully built kerMIT\n",
            "Installing collected packages: kerMIT\n",
            "  Attempting uninstall: kerMIT\n",
            "    Found existing installation: kerMIT 2.0\n",
            "    Uninstalling kerMIT-2.0:\n",
            "      Successfully uninstalled kerMIT-2.0\n",
            "Successfully installed kerMIT-2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ./KERMIT/kerMIT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMOMsYTm1MsC",
        "outputId": "d8fd3a19-aaa2-417d-b8d1-0e70f06bf963"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset"
      ],
      "metadata": {
        "id": "56Ql5YPZQoM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "f4f430d0e55b4b2286a4adef10b8b4df",
            "512828c74c9d4c11aee92ca55ccf9d79",
            "5fb12a5ab1dd4c6d809bc085207e3ce1",
            "9237036b37e3430b8df863867e3ee6ff",
            "28ab29fea45c491692261f2b04f9aa1c",
            "d171848d0a2a4550ad1cb8198098c62a",
            "55225a11da7345df872c94a5a1184678",
            "936ae44243764e6dbd842e4776910f8d",
            "d7becc734f25441f8057437201de242b",
            "8f6aa4b3899149f3ac09dd8e3632b183",
            "5b395d6139164f3ca1215bb9aa28cb0e"
          ]
        },
        "id": "4OrI5RCOex9a",
        "outputId": "253d6e39-0d2a-4eb3-bcc0-b0bba8976c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   ‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4f430d0e55b4b2286a4adef10b8b4df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:stanza:Language en package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "======================================\n",
            "| Processor    | Package             |\n",
            "--------------------------------------\n",
            "| tokenize     | combined            |\n",
            "| mwt          | combined            |\n",
            "| pos          | combined_charlm     |\n",
            "| constituency | ptb3-revised_charlm |\n",
            "======================================\n",
            "\n",
            "INFO:stanza:Using device: cuda\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: constituency\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "#download and install parser (stanfordcorenlp is the library that was build before stanza, mostly written in java)\n",
        "#!pip install stanfordcorenlp\n",
        "#!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
        "#!unzip stanford-corenlp-full-2018-10-05.zip\n",
        "\n",
        "\n",
        "import stanza\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,pos,constituency')\n",
        "\n",
        "ERRORS = []\n",
        "\n",
        "#takes sentences and parse freetext in parenthetical tree\n",
        "def parse(text):\n",
        "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            doc = nlp(text)\n",
        "        except Exception:\n",
        "            ERRORS.append(text)\n",
        "            return \"(S)\"\n",
        "\n",
        "        senteces = doc.sentences\n",
        "        #check if there are more than one sentence\n",
        "        if len(senteces) <= 1:\n",
        "            root = str(senteces[0].constituency)\n",
        "        else:\n",
        "            s1 = str(senteces[0].constituency)\n",
        "            root = \"(S\" + s1\n",
        "            for sentence in senteces[1:]:\n",
        "                s2 = str(sentence.constituency)\n",
        "                root = root + s2\n",
        "            root = root + \")\"\n",
        "        return root\n",
        "    except Exception:\n",
        "        ERRORS.append(text)\n",
        "        return \"(S)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split={'train':'train', 'test':'validation'})\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX_Cgjg30lUJ",
        "outputId": "50fb7cd4-dcb4-45f9-fd4c-092c3d62caf8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAmnHbfhf4jj",
        "outputId": "020fcbb4-281e-4d14-da5c-1361bc592096"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = {'train':4000, 'test': 400}\n",
        "for split in ['train', 'test']:\n",
        "    dataset[split] = dataset[split].select([i for i in range(max_len[split])])\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPSfHKOE3H3l",
        "outputId": "2fbcf207-1a87-4140-83d3-4f143d98116b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 400\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.sum(np.array(dataset['train']['label']) == 0), np.sum(np.array(dataset['test']['label']) == 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPgAe2YK167N",
        "outputId": "b844f3da-3045-4862-b6ce-969722394385"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1806, 189)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse"
      ],
      "metadata": {
        "id": "zaP3IhxUbT5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from kerMIT.tree_encode import parse as parse_tree # working with stanfordcorenlp\n",
        "\n",
        "def parse_example(example):\n",
        "    example['ptext'] = str(parse(example['sentence'])) #parse_tree\n",
        "    return example\n",
        "\n",
        "\n",
        "for split in ['train', 'test']:\n",
        "    dataset[split] = dataset[split].map(parse_example)\n"
      ],
      "metadata": {
        "id": "9U3UsBmA1ymv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distributed trees"
      ],
      "metadata": {
        "id": "96hC_v_kbOQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TREE_DIM = 512\n",
        "LAMBDA = 0.5\n",
        "\n",
        "\n",
        "from kerMIT.dtk import DT\n",
        "from kerMIT.operation import fast_shuffled_convolution\n",
        "from kerMIT import tree\n",
        "\n",
        "# encoder: takes a tree as input and compute the corresponding distributed tree\n",
        "encoder = DT(dimension=TREE_DIM, LAMBDA=LAMBDA, operation=fast_shuffled_convolution)\n",
        "\n",
        "\n",
        "def compute_distributed_tree(example):\n",
        "    t = tree.Tree(string=example['ptext'])\n",
        "    dt = encoder.dt(t)\n",
        "    example['t'] = str(t)\n",
        "    example['dt'] = dt\n",
        "    return example\n",
        "\n",
        "for split in ['train', 'test']:\n",
        "    dataset[split] = dataset[split].map(compute_distributed_tree, num_proc=100)"
      ],
      "metadata": {
        "id": "-UzGcV8Q8fa8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['dt'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5qe3RImHVQX",
        "outputId": "e432001e-7078-4fdd-822f-7211d0b52163"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.07816229648876002,\n",
              " -0.12890933334120983,\n",
              " 0.2620093034141668,\n",
              " 0.01991379638345948,\n",
              " -0.07938321376226146,\n",
              " -0.09590064283741555,\n",
              " 0.3057119261030242,\n",
              " -0.10928208209936344,\n",
              " -0.10972006449996644,\n",
              " 0.014288799846595382,\n",
              " -0.05853813740329025,\n",
              " -0.17327732506657614,\n",
              " -0.2973010818545604,\n",
              " 0.03300080294961305,\n",
              " 0.09740428053402841,\n",
              " 0.12257178673075415,\n",
              " 0.1795463632722691,\n",
              " -0.1818378618083274,\n",
              " 0.09959165299923207,\n",
              " 0.3658965469556189,\n",
              " -0.12397606496648025,\n",
              " -0.08892906468056819,\n",
              " -0.01721817995772821,\n",
              " -0.2660532126212299,\n",
              " -0.032545194133543544,\n",
              " -0.17080465252185242,\n",
              " -0.19080934970103625,\n",
              " -0.27546662786536463,\n",
              " 0.015066721016897964,\n",
              " 0.12688911228519084,\n",
              " 0.04595199586518084,\n",
              " 0.06543143887474986,\n",
              " 0.11983543093176302,\n",
              " -0.23105888476457273,\n",
              " 0.017716466703421547,\n",
              " -0.04497684451947864,\n",
              " -0.03191553474751091,\n",
              " 0.20387907888631882,\n",
              " 0.005661650706871725,\n",
              " -0.22031602872299336,\n",
              " -0.22270660376319423,\n",
              " -0.025994204561363192,\n",
              " -0.048455982348220544,\n",
              " 0.30011446707844625,\n",
              " 0.25696614981767363,\n",
              " -0.026745211518189725,\n",
              " -0.19860965962130123,\n",
              " -0.19827627749706833,\n",
              " 0.18841751571042512,\n",
              " 0.2615718320156328,\n",
              " -0.016636889441083845,\n",
              " 0.1476104788920475,\n",
              " 0.17084295704160496,\n",
              " 0.26317458941210875,\n",
              " 0.12065448953120775,\n",
              " 0.23808063854026784,\n",
              " 0.14089752506815384,\n",
              " -0.03182322889665275,\n",
              " 0.08298247506905003,\n",
              " -0.05973757088245299,\n",
              " 0.12798673425896662,\n",
              " 0.1032314643258962,\n",
              " -0.30223201088001483,\n",
              " -0.05821137735072342,\n",
              " -0.10794909828939678,\n",
              " 0.27915877201286493,\n",
              " -0.2986593975671425,\n",
              " -0.32487605270977793,\n",
              " -0.043398157734893826,\n",
              " 0.23658798244412316,\n",
              " 0.08003010363846118,\n",
              " -0.10161879779293542,\n",
              " -0.06463256459840257,\n",
              " -0.24004087974362662,\n",
              " 0.2310543374646984,\n",
              " 0.23346057989189753,\n",
              " -0.07592070293531065,\n",
              " -0.2483534120335934,\n",
              " -0.01275847098602656,\n",
              " 0.17233520087711102,\n",
              " 0.025962758265436536,\n",
              " -0.10855231700648346,\n",
              " 0.3078574317420829,\n",
              " -0.013269411691094114,\n",
              " 0.18960418612845215,\n",
              " 0.036926345306375376,\n",
              " 0.26622817023067574,\n",
              " 0.11952578161543409,\n",
              " -0.19447914186273124,\n",
              " -0.05448844163347888,\n",
              " -0.04580502439171932,\n",
              " 0.07176792982567734,\n",
              " 0.07263829318073309,\n",
              " -0.14267386716996053,\n",
              " -0.13978329682258547,\n",
              " -0.048270880544327585,\n",
              " -0.5185292933253195,\n",
              " 0.00657385114972487,\n",
              " -0.14086699089203045,\n",
              " 0.06920228573724696,\n",
              " 0.03810147624920622,\n",
              " -0.058552789980661324,\n",
              " -0.22883842284028216,\n",
              " 0.23541078477962418,\n",
              " 0.1282515706536492,\n",
              " 0.09274533978957639,\n",
              " 0.28883611382963015,\n",
              " 0.0026890077040316873,\n",
              " 0.09977733782134131,\n",
              " -0.2395370840145769,\n",
              " 0.03773987880445279,\n",
              " -0.25275958757958883,\n",
              " -0.11567382744001312,\n",
              " 0.015461009207088417,\n",
              " -0.07715427073980513,\n",
              " -0.24637716604891008,\n",
              " -0.2610418570487616,\n",
              " 0.08565657834039424,\n",
              " 0.19465327790092096,\n",
              " 0.23024230961036127,\n",
              " -0.23833181004178186,\n",
              " -0.041815850343981206,\n",
              " 0.17882676455271096,\n",
              " 0.06667211426006812,\n",
              " -0.29629500687443167,\n",
              " 0.027830139736410818,\n",
              " -0.1438928813537565,\n",
              " 0.06901836796090122,\n",
              " -0.028616692503546343,\n",
              " 0.010423272930391955,\n",
              " 0.18993773122635293,\n",
              " 0.4221513544280092,\n",
              " -0.12649685889058093,\n",
              " 0.020620034848707038,\n",
              " 0.19354395753672812,\n",
              " 0.2622046810210279,\n",
              " 0.1646090236143812,\n",
              " 0.4183062209429059,\n",
              " 0.05886193152926099,\n",
              " 0.08536406314244221,\n",
              " 0.04054485470974665,\n",
              " 0.20336018443126416,\n",
              " 0.012067620758882444,\n",
              " -0.14567180774015323,\n",
              " -0.1607344874840777,\n",
              " -0.06194879019403229,\n",
              " -0.02462524813148633,\n",
              " -0.19570133808159604,\n",
              " -0.1494796709494903,\n",
              " 0.12230772491011996,\n",
              " -0.09539641381625873,\n",
              " 0.07639654977154102,\n",
              " 0.06623279587171918,\n",
              " 0.06571502985182083,\n",
              " -0.07326834430011588,\n",
              " 0.024225693330280094,\n",
              " -0.009212839569114936,\n",
              " 0.1388815809170767,\n",
              " -0.033140711546234464,\n",
              " -0.07930456778522588,\n",
              " 0.056809613219239996,\n",
              " 0.0396977852418034,\n",
              " -0.02761093700264299,\n",
              " -0.02486513109211947,\n",
              " -0.02040051544463555,\n",
              " -0.04813212167887415,\n",
              " 0.012262716824525982,\n",
              " 0.1270195555624028,\n",
              " 0.026617346921069242,\n",
              " 0.12346840029290983,\n",
              " -0.060713050091371706,\n",
              " -0.2032407814132357,\n",
              " -0.2856489749829614,\n",
              " 0.0528087530433528,\n",
              " 0.07012602058482378,\n",
              " -0.18151565160003272,\n",
              " -0.21197614962834832,\n",
              " -0.26270522796731444,\n",
              " -0.042088103065244166,\n",
              " 0.2170936603314883,\n",
              " 0.034824333700166715,\n",
              " 0.25539732815182736,\n",
              " 0.0810601871958564,\n",
              " -0.024460466886171837,\n",
              " -0.2381083044455065,\n",
              " 0.2871025456183788,\n",
              " -0.010037939759170729,\n",
              " 0.19868286792021536,\n",
              " -0.24173469980494794,\n",
              " 0.009872610461364381,\n",
              " 0.2801656612118802,\n",
              " 0.10717992168611774,\n",
              " 0.16389349358256938,\n",
              " 0.11817527603651347,\n",
              " 0.1080411680642788,\n",
              " 0.05265025386427819,\n",
              " -0.260764049823992,\n",
              " -0.2607457087134864,\n",
              " -0.10812048535492974,\n",
              " -0.07609493041311399,\n",
              " -0.12834094976916238,\n",
              " -0.0063472308733824225,\n",
              " 0.012116852155697376,\n",
              " -0.19946877979531083,\n",
              " -0.27130995138821534,\n",
              " 0.1243131847200434,\n",
              " -0.057244984463261844,\n",
              " 0.015141544922003886,\n",
              " 0.06924008146087658,\n",
              " -0.2458099696411753,\n",
              " 0.06019255046690479,\n",
              " 0.09949521171793176,\n",
              " 0.08763048008794018,\n",
              " -0.025425374897534086,\n",
              " 0.10400046858006812,\n",
              " 0.11742493826920622,\n",
              " -0.16903461622068391,\n",
              " 0.5079178687118598,\n",
              " 0.18585708806475146,\n",
              " -0.011495262847595562,\n",
              " -0.04898300174944368,\n",
              " 0.21917270777497294,\n",
              " 0.24086684473031114,\n",
              " 0.24365576101262557,\n",
              " -0.11924751350203783,\n",
              " 0.32249959105858556,\n",
              " -0.28640741056987823,\n",
              " 0.1935685871580346,\n",
              " 0.038814194434128216,\n",
              " -0.377829132218124,\n",
              " -0.010452651111828096,\n",
              " -0.20476127308371583,\n",
              " 0.10744799604563818,\n",
              " 0.3816310076547955,\n",
              " -0.09359755505382523,\n",
              " -0.4083453186775425,\n",
              " 0.16981931516938897,\n",
              " 0.024953186405431378,\n",
              " 0.18234123058346857,\n",
              " -0.2752463676900988,\n",
              " -0.42994733882378433,\n",
              " 0.06775439608973337,\n",
              " 0.30554537596459047,\n",
              " 0.061257234757595064,\n",
              " 0.3135566693693682,\n",
              " 0.18375277491969863,\n",
              " -0.07832424509638235,\n",
              " -0.04154475904588556,\n",
              " -0.3951345257631352,\n",
              " 0.11447266995225659,\n",
              " -0.1030537357272897,\n",
              " -0.11287253057079563,\n",
              " -0.006602680800109353,\n",
              " -0.3952770105098315,\n",
              " 0.17375505647831746,\n",
              " -0.07762048333167851,\n",
              " 0.051917255455359475,\n",
              " -0.028537435838041823,\n",
              " 0.09683389413025614,\n",
              " -0.5219408473224467,\n",
              " 0.08263276509392943,\n",
              " -0.009386470066473439,\n",
              " 0.09683793513029036,\n",
              " 0.32113281442459524,\n",
              " -0.21248379241528154,\n",
              " -0.053126480511241705,\n",
              " -0.06384726697666865,\n",
              " -0.14459067168080958,\n",
              " 0.5285925600690362,\n",
              " -0.3168690134414594,\n",
              " 0.1944109613495951,\n",
              " -0.07436866254227989,\n",
              " 0.19378578097994553,\n",
              " 0.1353074220162718,\n",
              " 0.2674854327094114,\n",
              " 0.05020937937961725,\n",
              " -0.057652508664478005,\n",
              " 0.11851199485081479,\n",
              " -0.04563285377983617,\n",
              " -0.28798899487120133,\n",
              " 0.02379380229924602,\n",
              " 0.09405387515041468,\n",
              " 0.17022115295137763,\n",
              " -0.04379607425443088,\n",
              " -0.22626486744749003,\n",
              " -0.04227292468245057,\n",
              " 0.17316389333788645,\n",
              " 0.12203413864024638,\n",
              " -0.10481126706511755,\n",
              " -0.14506889577652354,\n",
              " 0.044056382514806536,\n",
              " 0.2522868852144351,\n",
              " 0.17237041092791344,\n",
              " -0.12216325569588188,\n",
              " -0.18483590792183682,\n",
              " -0.19554554853658862,\n",
              " -0.21605460180984626,\n",
              " -0.11907707001685736,\n",
              " -0.05007718657292854,\n",
              " -0.034659952747726,\n",
              " 0.09854665890028491,\n",
              " -0.1606814675005095,\n",
              " 0.16318412139355454,\n",
              " -0.2310269501113651,\n",
              " -0.23129757689388314,\n",
              " -0.22672301408962126,\n",
              " 0.1690216786518582,\n",
              " -0.24818633952089128,\n",
              " -0.3220821834226665,\n",
              " 0.2968592074850298,\n",
              " -0.2363798072554811,\n",
              " 0.3237334997044406,\n",
              " 0.026549654466276773,\n",
              " -0.10047296069661649,\n",
              " 0.20960312500228115,\n",
              " 0.22999556370391327,\n",
              " -0.36597795579949205,\n",
              " -0.014235646267387221,\n",
              " 0.042433610286428176,\n",
              " 0.03905436833938816,\n",
              " -0.15788982039386268,\n",
              " -0.03192142250270402,\n",
              " 0.3104536944105446,\n",
              " 0.15213305227382673,\n",
              " 0.2811491189362477,\n",
              " -0.07376089343367936,\n",
              " -0.009450500184437032,\n",
              " -0.018870336837506,\n",
              " -0.01013890149256065,\n",
              " -0.2964021554733206,\n",
              " 0.015311878445320537,\n",
              " 0.1809851645608546,\n",
              " -0.16486509524722381,\n",
              " 0.19213623750051245,\n",
              " -0.14287652077555452,\n",
              " 0.20867657876789283,\n",
              " 0.0677103514205499,\n",
              " 0.019752850407980478,\n",
              " -0.2450560275710941,\n",
              " 0.020330761101033035,\n",
              " 0.20693619070978742,\n",
              " -0.027439623736893994,\n",
              " -0.41613487897139945,\n",
              " -0.1982729780792712,\n",
              " 0.28198053281159385,\n",
              " -0.12627649983174388,\n",
              " 0.03933338875696133,\n",
              " -0.4204926005851456,\n",
              " 0.14191650953717322,\n",
              " -0.13754251375441917,\n",
              " 0.48356885355325574,\n",
              " 0.043909628357607,\n",
              " -0.39666752151879253,\n",
              " -0.0017132592450089176,\n",
              " -0.21040341283716196,\n",
              " -0.09978059194778359,\n",
              " -0.24160219775911998,\n",
              " -0.03172542252005764,\n",
              " 0.11611580094982792,\n",
              " 0.14828545332261717,\n",
              " 0.06727517472572453,\n",
              " -0.02808856719185688,\n",
              " 0.3549751688304599,\n",
              " -0.03468044398645271,\n",
              " -0.12703381557897817,\n",
              " 0.17165759504991343,\n",
              " -0.007977845656641502,\n",
              " 0.015832120002261738,\n",
              " -0.13430164294454408,\n",
              " -0.22449746443623927,\n",
              " 0.19609750794903005,\n",
              " -0.08774225574328678,\n",
              " 0.14959586256320062,\n",
              " -0.2208325023053725,\n",
              " 0.02177970020186252,\n",
              " 0.37703743833032277,\n",
              " -0.052005153630288854,\n",
              " -0.38664297229738326,\n",
              " -0.04539840289989849,\n",
              " 0.2571208142070539,\n",
              " 0.32163105376960877,\n",
              " -0.155284646756614,\n",
              " -0.14042593871161455,\n",
              " 0.23924405282769706,\n",
              " 0.33070486201489324,\n",
              " -0.16054283820705328,\n",
              " 0.28206178630858514,\n",
              " 0.2824817746564197,\n",
              " 0.21978356740011343,\n",
              " -0.19605701186373028,\n",
              " 0.2820213395411361,\n",
              " 0.21669489042791173,\n",
              " -0.12295452520997387,\n",
              " -0.3103417382699933,\n",
              " 0.24001863742989332,\n",
              " 0.40159016424301613,\n",
              " 0.22028997021203475,\n",
              " 0.04659660306415603,\n",
              " -0.31209568137858534,\n",
              " 0.010031649332876643,\n",
              " 0.10933187528210332,\n",
              " -0.03734589703014379,\n",
              " -0.3258901365617584,\n",
              " -0.14513160537517958,\n",
              " 0.20258865800736914,\n",
              " -0.11028924226186035,\n",
              " -0.08376249942622106,\n",
              " -0.105792158277391,\n",
              " 0.19470952969984634,\n",
              " -0.17629639630284827,\n",
              " 0.04667268645928637,\n",
              " -0.08729857389607928,\n",
              " 0.24078591954709683,\n",
              " -0.04592515550074883,\n",
              " -0.3159236106352069,\n",
              " 0.483585470367447,\n",
              " -0.369049554578245,\n",
              " 0.3528150935777004,\n",
              " 0.188134538030203,\n",
              " 0.3899536971681057,\n",
              " 0.16651775448461603,\n",
              " -0.11276561506587418,\n",
              " 0.0960382411936543,\n",
              " -0.16489471626032098,\n",
              " -0.2239877614700467,\n",
              " 0.09897066616278295,\n",
              " -0.14329751783684036,\n",
              " 0.23803279050273704,\n",
              " 0.04967281946573291,\n",
              " -0.2362293895690419,\n",
              " -0.08702303852449406,\n",
              " 0.12045525774060119,\n",
              " 0.23699733684706992,\n",
              " -0.035713131409030506,\n",
              " 0.031394299564565784,\n",
              " -0.23969608261216613,\n",
              " -0.13404330003792259,\n",
              " 0.07952830740913056,\n",
              " 0.13097604453656847,\n",
              " 0.08632423734638667,\n",
              " 0.037748016156116115,\n",
              " 0.15161987353469183,\n",
              " -0.11520997770530261,\n",
              " -0.00641198074532789,\n",
              " -0.09132800690903901,\n",
              " 0.008163429903573098,\n",
              " -0.18342853825381997,\n",
              " 0.1566642923435894,\n",
              " 0.06329057765752953,\n",
              " 0.3198211722809718,\n",
              " -0.14957813141535078,\n",
              " 0.09535653675570291,\n",
              " -0.14642547862030736,\n",
              " -0.02154536209122263,\n",
              " -0.09771812162854228,\n",
              " 0.1601119905512212,\n",
              " 0.1218536362938912,\n",
              " -0.20969973370276596,\n",
              " 0.14597583701827407,\n",
              " -0.1162155592895901,\n",
              " -0.02052900251825052,\n",
              " 0.028619438701347116,\n",
              " -0.2529159831368561,\n",
              " 0.29338276854272227,\n",
              " -0.3081435652150182,\n",
              " -0.03769593921261344,\n",
              " 0.033619505055245275,\n",
              " 0.0398304987714873,\n",
              " 0.08667367881030244,\n",
              " 0.18824387825760022,\n",
              " -0.010551135616971116,\n",
              " 0.10915843449863384,\n",
              " -0.24628523792404097,\n",
              " 0.002076206474646996,\n",
              " -0.1892537874985783,\n",
              " 0.18882800278618847,\n",
              " 0.12910442883272072,\n",
              " 0.0005824754807867288,\n",
              " -0.08133294627426307,\n",
              " -0.19647236192888357,\n",
              " -0.18347717068668049,\n",
              " -0.08418651197382149,\n",
              " -0.11831200046848891,\n",
              " -0.2402584938305002,\n",
              " -0.04487771126033454,\n",
              " 0.1977042872411437,\n",
              " -0.22265088004810663,\n",
              " 0.19743953658460928,\n",
              " -0.13761773046476625,\n",
              " 0.049001156117179784,\n",
              " -0.2369450541218589,\n",
              " -0.4971386727950111,\n",
              " -0.06279182808328694,\n",
              " -0.012465831848832522,\n",
              " -0.09859777068844647,\n",
              " 0.31938905334967493,\n",
              " -0.03281826166504448,\n",
              " -0.19873395199631974,\n",
              " -0.05856289235818034,\n",
              " -0.08559632194203048,\n",
              " -0.057654100001559815,\n",
              " 0.06473951762243155,\n",
              " -0.06780032526197595,\n",
              " 0.1349999574260582,\n",
              " 0.15186407406809888,\n",
              " 0.02688480736165405,\n",
              " 0.10148902548243492,\n",
              " 0.15178369393154978,\n",
              " 0.02692685535674181,\n",
              " -0.007928156120207482,\n",
              " 0.19867635606311693,\n",
              " 0.20626811709806328]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ERRORS), dataset['train'][0])\n",
        "display(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xeSSRT0T7Q2T",
        "outputId": "2b96e6c0-befc-4393-e98e-9eddac70ea4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 {'sentence': 'klein , charming in comedies like american pie and dead-on in election , ', 'label': 1, 'idx': 32326, 'ptext': '(ROOT (FRAG (ADJP (NNP klein)) (, ,) (ADJP (JJ charming) (PP (IN in) (NP (NP (NNS comedies)) (PP (IN like) (NP (JJ american) (NN pie)))))) (CC and) (ADJP (JJ dead) (HYPH -) (IN on)) (PP (IN in) (NP (NN election))) (, ,)))', 't': '(ROOT (FRAG (ADJP (NNP klein)) (, ,) (ADJP (JJ charming) (PP (IN in) (NP (NP (NNS comedies)) (PP (IN like) (NP (JJ american) (NN pie)))))) (CC and) (ADJP (JJ dead) (HYPH -) (IN on)) (PP (IN in) (NP (NN election))) (, ,)))', 'dt': [-0.07816229648876002, -0.12890933334120983, 0.2620093034141668, 0.01991379638345948, -0.07938321376226146, -0.09590064283741555, 0.3057119261030242, -0.10928208209936344, -0.10972006449996644, 0.014288799846595382, -0.05853813740329025, -0.17327732506657614, -0.2973010818545604, 0.03300080294961305, 0.09740428053402841, 0.12257178673075415, 0.1795463632722691, -0.1818378618083274, 0.09959165299923207, 0.3658965469556189, -0.12397606496648025, -0.08892906468056819, -0.01721817995772821, -0.2660532126212299, -0.032545194133543544, -0.17080465252185242, -0.19080934970103625, -0.27546662786536463, 0.015066721016897964, 0.12688911228519084, 0.04595199586518084, 0.06543143887474986, 0.11983543093176302, -0.23105888476457273, 0.017716466703421547, -0.04497684451947864, -0.03191553474751091, 0.20387907888631882, 0.005661650706871725, -0.22031602872299336, -0.22270660376319423, -0.025994204561363192, -0.048455982348220544, 0.30011446707844625, 0.25696614981767363, -0.026745211518189725, -0.19860965962130123, -0.19827627749706833, 0.18841751571042512, 0.2615718320156328, -0.016636889441083845, 0.1476104788920475, 0.17084295704160496, 0.26317458941210875, 0.12065448953120775, 0.23808063854026784, 0.14089752506815384, -0.03182322889665275, 0.08298247506905003, -0.05973757088245299, 0.12798673425896662, 0.1032314643258962, -0.30223201088001483, -0.05821137735072342, -0.10794909828939678, 0.27915877201286493, -0.2986593975671425, -0.32487605270977793, -0.043398157734893826, 0.23658798244412316, 0.08003010363846118, -0.10161879779293542, -0.06463256459840257, -0.24004087974362662, 0.2310543374646984, 0.23346057989189753, -0.07592070293531065, -0.2483534120335934, -0.01275847098602656, 0.17233520087711102, 0.025962758265436536, -0.10855231700648346, 0.3078574317420829, -0.013269411691094114, 0.18960418612845215, 0.036926345306375376, 0.26622817023067574, 0.11952578161543409, -0.19447914186273124, -0.05448844163347888, -0.04580502439171932, 0.07176792982567734, 0.07263829318073309, -0.14267386716996053, -0.13978329682258547, -0.048270880544327585, -0.5185292933253195, 0.00657385114972487, -0.14086699089203045, 0.06920228573724696, 0.03810147624920622, -0.058552789980661324, -0.22883842284028216, 0.23541078477962418, 0.1282515706536492, 0.09274533978957639, 0.28883611382963015, 0.0026890077040316873, 0.09977733782134131, -0.2395370840145769, 0.03773987880445279, -0.25275958757958883, -0.11567382744001312, 0.015461009207088417, -0.07715427073980513, -0.24637716604891008, -0.2610418570487616, 0.08565657834039424, 0.19465327790092096, 0.23024230961036127, -0.23833181004178186, -0.041815850343981206, 0.17882676455271096, 0.06667211426006812, -0.29629500687443167, 0.027830139736410818, -0.1438928813537565, 0.06901836796090122, -0.028616692503546343, 0.010423272930391955, 0.18993773122635293, 0.4221513544280092, -0.12649685889058093, 0.020620034848707038, 0.19354395753672812, 0.2622046810210279, 0.1646090236143812, 0.4183062209429059, 0.05886193152926099, 0.08536406314244221, 0.04054485470974665, 0.20336018443126416, 0.012067620758882444, -0.14567180774015323, -0.1607344874840777, -0.06194879019403229, -0.02462524813148633, -0.19570133808159604, -0.1494796709494903, 0.12230772491011996, -0.09539641381625873, 0.07639654977154102, 0.06623279587171918, 0.06571502985182083, -0.07326834430011588, 0.024225693330280094, -0.009212839569114936, 0.1388815809170767, -0.033140711546234464, -0.07930456778522588, 0.056809613219239996, 0.0396977852418034, -0.02761093700264299, -0.02486513109211947, -0.02040051544463555, -0.04813212167887415, 0.012262716824525982, 0.1270195555624028, 0.026617346921069242, 0.12346840029290983, -0.060713050091371706, -0.2032407814132357, -0.2856489749829614, 0.0528087530433528, 0.07012602058482378, -0.18151565160003272, -0.21197614962834832, -0.26270522796731444, -0.042088103065244166, 0.2170936603314883, 0.034824333700166715, 0.25539732815182736, 0.0810601871958564, -0.024460466886171837, -0.2381083044455065, 0.2871025456183788, -0.010037939759170729, 0.19868286792021536, -0.24173469980494794, 0.009872610461364381, 0.2801656612118802, 0.10717992168611774, 0.16389349358256938, 0.11817527603651347, 0.1080411680642788, 0.05265025386427819, -0.260764049823992, -0.2607457087134864, -0.10812048535492974, -0.07609493041311399, -0.12834094976916238, -0.0063472308733824225, 0.012116852155697376, -0.19946877979531083, -0.27130995138821534, 0.1243131847200434, -0.057244984463261844, 0.015141544922003886, 0.06924008146087658, -0.2458099696411753, 0.06019255046690479, 0.09949521171793176, 0.08763048008794018, -0.025425374897534086, 0.10400046858006812, 0.11742493826920622, -0.16903461622068391, 0.5079178687118598, 0.18585708806475146, -0.011495262847595562, -0.04898300174944368, 0.21917270777497294, 0.24086684473031114, 0.24365576101262557, -0.11924751350203783, 0.32249959105858556, -0.28640741056987823, 0.1935685871580346, 0.038814194434128216, -0.377829132218124, -0.010452651111828096, -0.20476127308371583, 0.10744799604563818, 0.3816310076547955, -0.09359755505382523, -0.4083453186775425, 0.16981931516938897, 0.024953186405431378, 0.18234123058346857, -0.2752463676900988, -0.42994733882378433, 0.06775439608973337, 0.30554537596459047, 0.061257234757595064, 0.3135566693693682, 0.18375277491969863, -0.07832424509638235, -0.04154475904588556, -0.3951345257631352, 0.11447266995225659, -0.1030537357272897, -0.11287253057079563, -0.006602680800109353, -0.3952770105098315, 0.17375505647831746, -0.07762048333167851, 0.051917255455359475, -0.028537435838041823, 0.09683389413025614, -0.5219408473224467, 0.08263276509392943, -0.009386470066473439, 0.09683793513029036, 0.32113281442459524, -0.21248379241528154, -0.053126480511241705, -0.06384726697666865, -0.14459067168080958, 0.5285925600690362, -0.3168690134414594, 0.1944109613495951, -0.07436866254227989, 0.19378578097994553, 0.1353074220162718, 0.2674854327094114, 0.05020937937961725, -0.057652508664478005, 0.11851199485081479, -0.04563285377983617, -0.28798899487120133, 0.02379380229924602, 0.09405387515041468, 0.17022115295137763, -0.04379607425443088, -0.22626486744749003, -0.04227292468245057, 0.17316389333788645, 0.12203413864024638, -0.10481126706511755, -0.14506889577652354, 0.044056382514806536, 0.2522868852144351, 0.17237041092791344, -0.12216325569588188, -0.18483590792183682, -0.19554554853658862, -0.21605460180984626, -0.11907707001685736, -0.05007718657292854, -0.034659952747726, 0.09854665890028491, -0.1606814675005095, 0.16318412139355454, -0.2310269501113651, -0.23129757689388314, -0.22672301408962126, 0.1690216786518582, -0.24818633952089128, -0.3220821834226665, 0.2968592074850298, -0.2363798072554811, 0.3237334997044406, 0.026549654466276773, -0.10047296069661649, 0.20960312500228115, 0.22999556370391327, -0.36597795579949205, -0.014235646267387221, 0.042433610286428176, 0.03905436833938816, -0.15788982039386268, -0.03192142250270402, 0.3104536944105446, 0.15213305227382673, 0.2811491189362477, -0.07376089343367936, -0.009450500184437032, -0.018870336837506, -0.01013890149256065, -0.2964021554733206, 0.015311878445320537, 0.1809851645608546, -0.16486509524722381, 0.19213623750051245, -0.14287652077555452, 0.20867657876789283, 0.0677103514205499, 0.019752850407980478, -0.2450560275710941, 0.020330761101033035, 0.20693619070978742, -0.027439623736893994, -0.41613487897139945, -0.1982729780792712, 0.28198053281159385, -0.12627649983174388, 0.03933338875696133, -0.4204926005851456, 0.14191650953717322, -0.13754251375441917, 0.48356885355325574, 0.043909628357607, -0.39666752151879253, -0.0017132592450089176, -0.21040341283716196, -0.09978059194778359, -0.24160219775911998, -0.03172542252005764, 0.11611580094982792, 0.14828545332261717, 0.06727517472572453, -0.02808856719185688, 0.3549751688304599, -0.03468044398645271, -0.12703381557897817, 0.17165759504991343, -0.007977845656641502, 0.015832120002261738, -0.13430164294454408, -0.22449746443623927, 0.19609750794903005, -0.08774225574328678, 0.14959586256320062, -0.2208325023053725, 0.02177970020186252, 0.37703743833032277, -0.052005153630288854, -0.38664297229738326, -0.04539840289989849, 0.2571208142070539, 0.32163105376960877, -0.155284646756614, -0.14042593871161455, 0.23924405282769706, 0.33070486201489324, -0.16054283820705328, 0.28206178630858514, 0.2824817746564197, 0.21978356740011343, -0.19605701186373028, 0.2820213395411361, 0.21669489042791173, -0.12295452520997387, -0.3103417382699933, 0.24001863742989332, 0.40159016424301613, 0.22028997021203475, 0.04659660306415603, -0.31209568137858534, 0.010031649332876643, 0.10933187528210332, -0.03734589703014379, -0.3258901365617584, -0.14513160537517958, 0.20258865800736914, -0.11028924226186035, -0.08376249942622106, -0.105792158277391, 0.19470952969984634, -0.17629639630284827, 0.04667268645928637, -0.08729857389607928, 0.24078591954709683, -0.04592515550074883, -0.3159236106352069, 0.483585470367447, -0.369049554578245, 0.3528150935777004, 0.188134538030203, 0.3899536971681057, 0.16651775448461603, -0.11276561506587418, 0.0960382411936543, -0.16489471626032098, -0.2239877614700467, 0.09897066616278295, -0.14329751783684036, 0.23803279050273704, 0.04967281946573291, -0.2362293895690419, -0.08702303852449406, 0.12045525774060119, 0.23699733684706992, -0.035713131409030506, 0.031394299564565784, -0.23969608261216613, -0.13404330003792259, 0.07952830740913056, 0.13097604453656847, 0.08632423734638667, 0.037748016156116115, 0.15161987353469183, -0.11520997770530261, -0.00641198074532789, -0.09132800690903901, 0.008163429903573098, -0.18342853825381997, 0.1566642923435894, 0.06329057765752953, 0.3198211722809718, -0.14957813141535078, 0.09535653675570291, -0.14642547862030736, -0.02154536209122263, -0.09771812162854228, 0.1601119905512212, 0.1218536362938912, -0.20969973370276596, 0.14597583701827407, -0.1162155592895901, -0.02052900251825052, 0.028619438701347116, -0.2529159831368561, 0.29338276854272227, -0.3081435652150182, -0.03769593921261344, 0.033619505055245275, 0.0398304987714873, 0.08667367881030244, 0.18824387825760022, -0.010551135616971116, 0.10915843449863384, -0.24628523792404097, 0.002076206474646996, -0.1892537874985783, 0.18882800278618847, 0.12910442883272072, 0.0005824754807867288, -0.08133294627426307, -0.19647236192888357, -0.18347717068668049, -0.08418651197382149, -0.11831200046848891, -0.2402584938305002, -0.04487771126033454, 0.1977042872411437, -0.22265088004810663, 0.19743953658460928, -0.13761773046476625, 0.049001156117179784, -0.2369450541218589, -0.4971386727950111, -0.06279182808328694, -0.012465831848832522, -0.09859777068844647, 0.31938905334967493, -0.03281826166504448, -0.19873395199631974, -0.05856289235818034, -0.08559632194203048, -0.057654100001559815, 0.06473951762243155, -0.06780032526197595, 0.1349999574260582, 0.15186407406809888, 0.02688480736165405, 0.10148902548243492, 0.15178369393154978, 0.02692685535674181, -0.007928156120207482, 0.19867635606311693, 0.20626811709806328]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'label', 'idx', 'ptext', 't', 'dt'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'label', 'idx', 'ptext', 't', 'dt'],\n",
              "        num_rows: 400\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_target(example):\n",
        "    example['target'] = [0.0, 0.0]\n",
        "    example['target'][example['label']] = 1.0\n",
        "    return example\n",
        "\n",
        "for split in ['train', 'test']:\n",
        "    dataset[split] = dataset[split].map(label_to_target, num_proc=100)"
      ],
      "metadata": {
        "id": "rsR2TZGrL_3i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train'].select(list(range(1800))).with_format('torch')\n",
        "val_dataset = dataset['train'].select(list(range(1800,2000))).with_format('torch')\n",
        "test_dataset  = dataset['test'].with_format('torch')"
      ],
      "metadata": {
        "id": "HeSH-RfEI2y-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['target'].shape, train_dataset['dt'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSIq3HtoMh_m",
        "outputId": "62d9602c-517e-4286-c9b4-d5b632d63a6a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1800, 2]), torch.Size([1800, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "import torch\n",
        "\n",
        "def create_dataloader(dataset, **kwargs):\n",
        "    if 'batch_size' in kwargs:\n",
        "        batch_size = kwargs['batch_size']\n",
        "    else:\n",
        "        batch_size = 16\n",
        "\n",
        "    # Create the DataLoaders for our training and validation sets.\n",
        "    # We'll take training samples in random order.\n",
        "    dataloader = DataLoader(\n",
        "        dataset,  # The training samples.\n",
        "        sampler = SequentialSampler(dataset), # Select batches IN SEQUENCE NB: usually there is a Random Loader for train\n",
        "        batch_size = batch_size, # batch size in split\n",
        "        generator= torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = create_dataloader(train_dataset)\n",
        "val_dataloader = create_dataloader(val_dataset)\n",
        "test_dataloader = create_dataloader(test_dataset)"
      ],
      "metadata": {
        "id": "5qr5AhCbIzRH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "v2hM5_QHQv6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "#set manual seed for replicability\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(23)\n",
        "\n",
        "# If there's a GPU available...\n",
        "DEVICE_NUM = 0\n",
        "def load_device(DEVICE_NUM):\n",
        "    if torch.cuda.is_available():\n",
        "\n",
        "        # Tell PyTorch to use the GPU.\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(DEVICE_NUM))\n",
        "\n",
        "    # If not...\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.')\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    return device\n",
        "\n",
        "device = load_device(DEVICE_NUM)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIzl0gfPL54N",
        "outputId": "092c19b5-97a0-4d87-f3e6-763e80bda6d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIM = 2\n",
        "HIDDEN_SIZE = 128"
      ],
      "metadata": {
        "id": "y4EISMT6NndW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear_1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear_2(out)\n",
        "        return out\n",
        "\n",
        "    def get_activations(self, x):\n",
        "        with torch.no_grad():\n",
        "            A = [x] + [None] * 2 #(num linear layers)\n",
        "\n",
        "            out = self.linear_1(x)\n",
        "            out = self.relu(out)\n",
        "            A[1] = out\n",
        "\n",
        "            out = self.linear_2(out)\n",
        "            A[2] = out\n",
        "            return A\n",
        "\n",
        "model = FeedForward(TREE_DIM, OUTPUT_DIM, HIDDEN_SIZE)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr9uJB6LNjtP",
        "outputId": "71ff3212-f938-4080-f280-30d2a68bdd6e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedForward(\n",
              "  (linear_1): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (linear_2): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "C_G4Sn1xQyyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "METRICS = {\n",
        "    'sentiment': lambda y_true, y_pred: {\n",
        "        \"f1_score\": f1_score(y_true, y_pred),\n",
        "        \"accuracy_score\":accuracy_score(y_true, y_pred)\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "03SOnlm8N0PB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "from transformers import AdamW\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "def get_optim(model, **kwargs):\n",
        "    if 'lr' in kwargs:\n",
        "        lr = kwargs['lr']\n",
        "    else:\n",
        "        lr = 5e-5\n",
        "\n",
        "    if 'eps' in kwargs:\n",
        "        eps = kwargs['eps']\n",
        "    else:\n",
        "        eps = 1e-8\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                      lr=5e-5,  # args.learning_rate - default is 5e-5\n",
        "                      eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                      )\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def schedule(optimizer, epochs, train_dataloader):\n",
        "    # Total number of training steps is [number of batches] x [number of epochs].\n",
        "    # (Note that this is not the same as the number of training samples).\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Create the learning rate scheduler.\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                                                num_training_steps=total_steps)\n",
        "    return scheduler\n",
        "\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "def get_criterion():\n",
        "    return nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "XfFQkzrRN52U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(metric, logits, labels, return_preds=False):\n",
        "    # Get predictions from the maximum value\n",
        "    pred_flat = np.argmax(logits.cpu().numpy(), axis=1).flatten()\n",
        "    labels_flat = np.argmax(labels.cpu().numpy(), axis=1).flatten()\n",
        "\n",
        "    if not return_preds:\n",
        "        return metric(labels_flat, pred_flat)\n",
        "    else:\n",
        "        return metric(labels_flat, pred_flat), pred_flat, labels_flat\n",
        "\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "def training_loop(model, epochs, optimizer, scheduler, criterion, train_dataloader, validation_dataloader):\n",
        "    model.train()\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = 42\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # We'll store a number of quantities such as training and validation loss,\n",
        "    # validation accuracy, and timings.\n",
        "    training_stats = []\n",
        "\n",
        "    # Measure the total training time for the whole run.\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, epochs):\n",
        "\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "\n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "        total_train_score = {}\n",
        "\n",
        "        # Put the model into training mode. Don't be mislead--the call to\n",
        "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "        # `dropout` and `batchnorm` layers behave differently during training\n",
        "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                # Calculate elapsed time in minutes.\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "\n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            # Unpack this training batch from our dataloader.\n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "            # `to` method.\n",
        "            #\n",
        "            # `batch` contains pytorch tensors:\n",
        "            b_dt = batch['dt'].to(device)\n",
        "            b_labels = batch['target'].float().to(device)\n",
        "\n",
        "            # print(b_dt.shape, b_labels.shape)\n",
        "\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. PyTorch doesn't do this automatically because\n",
        "            # accumulating the gradients is \"convenient while training RNNs\".\n",
        "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "            # function and pass down the arguments.\n",
        "            logits = model(b_dt)\n",
        "            #print(\"logits\", logits)\n",
        "\n",
        "            # calculate loss (sigmoid on logits, then compare with targets)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value\n",
        "            # from the tensor.\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "            logits = logits.detach().cpu()\n",
        "            label_ids = b_labels.to('cpu')\n",
        "\n",
        "            score = compute_score(METRICS['sentiment'], logits, b_labels)\n",
        "            for m, v in score.items():\n",
        "                if m not in total_train_score:\n",
        "                    total_train_score[m] = 0\n",
        "                total_train_score[m] += v\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "        # Report the final accuracy for this train run.\n",
        "        for m in total_train_score:\n",
        "            total_train_score[m] = total_train_score[m] / len(train_dataloader)\n",
        "            print(\"  {metric}: {total_score:.2f}\".format(metric=m, total_score=total_train_score[m]))\n",
        "\n",
        "\n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        total_score = {}\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "\n",
        "            b_dt = batch['dt'].to(device)\n",
        "            b_labels = batch['target'].float().to(device)\n",
        "\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. PyTorch doesn't do this automatically because\n",
        "            # accumulating the gradients is \"convenient while training RNNs\".\n",
        "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "            # function and pass down the arguments.\n",
        "            with torch.no_grad():\n",
        "                logits = model(b_dt)\n",
        "\n",
        "            # calculate loss (sigmoid on logits, then compare with targets)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu()\n",
        "            label_ids = b_labels.to('cpu')\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            score = compute_score(METRICS['sentiment'], logits, label_ids)\n",
        "            for m, v in score.items():\n",
        "                if m not in total_score:\n",
        "                    total_score[m] = 0\n",
        "                total_score[m] += v\n",
        "\n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        for m in total_score:\n",
        "            total_score[m] = total_score[m] / len(validation_dataloader)\n",
        "            print(\"  {metric}: {total_score:.2f}\".format(metric=m, total_score=total_score[m]))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        stat = {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "        for m in total_train_score:\n",
        "            stat[\"Training \"+m] = total_train_score[m]\n",
        "\n",
        "        for m in total_score:\n",
        "            stat[\"Valid. \"+m] = total_score[m]\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(stat)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "    df_stats = pd.DataFrame(data=training_stats)\n",
        "    df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "    return df_stats"
      ],
      "metadata": {
        "id": "6sDg8_7AOADe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "optimizer = get_optim(model)\n",
        "scheduler = schedule(optimizer, EPOCHS, train_dataloader)\n",
        "criterion = get_criterion()\n",
        "\n",
        "train_stats = training_loop(model, EPOCHS, optimizer, scheduler, criterion,\n",
        "                            train_dataloader, val_dataloader)\n",
        "display(train_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4IP2KPmuOwAA",
        "outputId": "2df2bebc-eac0-473b-ca4c-0809e5d1a48c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:01.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.14\n",
            "  accuracy_score: 0.45\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.32\n",
            "  accuracy_score: 0.48\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:01.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.42\n",
            "  accuracy_score: 0.52\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.53\n",
            "  accuracy_score: 0.52\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:01.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:02.\n",
            "  f1_score: 0.63\n",
            "  accuracy_score: 0.59\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.63\n",
            "  accuracy_score: 0.53\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.70\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.66\n",
            "  accuracy_score: 0.55\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.63\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.67\n",
            "  accuracy_score: 0.54\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.69\n",
            "  accuracy_score: 0.57\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:00.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.71\n",
            "  accuracy_score: 0.59\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:00.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.60\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:00.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.60\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:00.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.60\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.59\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.62\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.59\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.63\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.59\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.63\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.59\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    113.    Elapsed: 0:00:01.\n",
            "  Batch    80  of    113.    Elapsed: 0:00:01.\n",
            "  f1_score: 0.73\n",
            "  accuracy_score: 0.63\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:01\n",
            "\n",
            "Running Validation...\n",
            "  f1_score: 0.72\n",
            "  accuracy_score: 0.59\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:17 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss Training Time Validation Time  \\\n",
              "epoch                                                             \n",
              "1           0.694705     0.693721       0:00:01         0:00:00   \n",
              "2           0.691939     0.691530       0:00:02         0:00:00   \n",
              "3           0.689560     0.689612       0:00:02         0:00:00   \n",
              "4           0.687345     0.687902       0:00:01         0:00:00   \n",
              "5           0.685241     0.686339       0:00:01         0:00:00   \n",
              "6           0.683236     0.684928       0:00:01         0:00:00   \n",
              "7           0.681348     0.683683       0:00:01         0:00:00   \n",
              "8           0.679599     0.682600       0:00:01         0:00:00   \n",
              "9           0.678003     0.681667       0:00:01         0:00:00   \n",
              "10          0.676585     0.680886       0:00:01         0:00:00   \n",
              "11          0.675366     0.680249       0:00:01         0:00:00   \n",
              "12          0.674361     0.679757       0:00:01         0:00:00   \n",
              "13          0.673584     0.679404       0:00:01         0:00:00   \n",
              "14          0.673043     0.679188       0:00:01         0:00:00   \n",
              "15          0.672743     0.679105       0:00:01         0:00:00   \n",
              "\n",
              "       Training f1_score  Training accuracy_score  Valid. f1_score  \\\n",
              "epoch                                                                \n",
              "1               0.139011                 0.452987         0.323652   \n",
              "2               0.416578                 0.516040         0.525828   \n",
              "3               0.627799                 0.591261         0.626079   \n",
              "4               0.697504                 0.617810         0.664732   \n",
              "5               0.721910                 0.626659         0.667960   \n",
              "6               0.724990                 0.620022         0.691076   \n",
              "7               0.727623                 0.620022         0.714589   \n",
              "8               0.727552                 0.617810         0.721662   \n",
              "9               0.731245                 0.622235         0.721196   \n",
              "10              0.730925                 0.622235         0.721196   \n",
              "11              0.730466                 0.621681         0.718763   \n",
              "12              0.732099                 0.625000         0.718763   \n",
              "13              0.732950                 0.626659         0.718763   \n",
              "14              0.732388                 0.626106         0.715213   \n",
              "15              0.733286                 0.627212         0.715213   \n",
              "\n",
              "       Valid. accuracy_score  \n",
              "epoch                         \n",
              "1                   0.480769  \n",
              "2                   0.524038  \n",
              "3                   0.533654  \n",
              "4                   0.548077  \n",
              "5                   0.543269  \n",
              "6                   0.567308  \n",
              "7                   0.591346  \n",
              "8                   0.600962  \n",
              "9                   0.596154  \n",
              "10                  0.596154  \n",
              "11                  0.591346  \n",
              "12                  0.591346  \n",
              "13                  0.591346  \n",
              "14                  0.586538  \n",
              "15                  0.586538  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45dcb2dc-9735-4baa-976f-a23bb735a7ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "      <th>Training f1_score</th>\n",
              "      <th>Training accuracy_score</th>\n",
              "      <th>Valid. f1_score</th>\n",
              "      <th>Valid. accuracy_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.694705</td>\n",
              "      <td>0.693721</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.139011</td>\n",
              "      <td>0.452987</td>\n",
              "      <td>0.323652</td>\n",
              "      <td>0.480769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.691939</td>\n",
              "      <td>0.691530</td>\n",
              "      <td>0:00:02</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.416578</td>\n",
              "      <td>0.516040</td>\n",
              "      <td>0.525828</td>\n",
              "      <td>0.524038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.689560</td>\n",
              "      <td>0.689612</td>\n",
              "      <td>0:00:02</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.627799</td>\n",
              "      <td>0.591261</td>\n",
              "      <td>0.626079</td>\n",
              "      <td>0.533654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.687345</td>\n",
              "      <td>0.687902</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.697504</td>\n",
              "      <td>0.617810</td>\n",
              "      <td>0.664732</td>\n",
              "      <td>0.548077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.685241</td>\n",
              "      <td>0.686339</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.721910</td>\n",
              "      <td>0.626659</td>\n",
              "      <td>0.667960</td>\n",
              "      <td>0.543269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.683236</td>\n",
              "      <td>0.684928</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.724990</td>\n",
              "      <td>0.620022</td>\n",
              "      <td>0.691076</td>\n",
              "      <td>0.567308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.681348</td>\n",
              "      <td>0.683683</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.727623</td>\n",
              "      <td>0.620022</td>\n",
              "      <td>0.714589</td>\n",
              "      <td>0.591346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.679599</td>\n",
              "      <td>0.682600</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.727552</td>\n",
              "      <td>0.617810</td>\n",
              "      <td>0.721662</td>\n",
              "      <td>0.600962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.678003</td>\n",
              "      <td>0.681667</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.731245</td>\n",
              "      <td>0.622235</td>\n",
              "      <td>0.721196</td>\n",
              "      <td>0.596154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.676585</td>\n",
              "      <td>0.680886</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.730925</td>\n",
              "      <td>0.622235</td>\n",
              "      <td>0.721196</td>\n",
              "      <td>0.596154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.675366</td>\n",
              "      <td>0.680249</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.730466</td>\n",
              "      <td>0.621681</td>\n",
              "      <td>0.718763</td>\n",
              "      <td>0.591346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.674361</td>\n",
              "      <td>0.679757</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.732099</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.718763</td>\n",
              "      <td>0.591346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.673584</td>\n",
              "      <td>0.679404</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.732950</td>\n",
              "      <td>0.626659</td>\n",
              "      <td>0.718763</td>\n",
              "      <td>0.591346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.673043</td>\n",
              "      <td>0.679188</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.732388</td>\n",
              "      <td>0.626106</td>\n",
              "      <td>0.715213</td>\n",
              "      <td>0.586538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.672743</td>\n",
              "      <td>0.679105</td>\n",
              "      <td>0:00:01</td>\n",
              "      <td>0:00:00</td>\n",
              "      <td>0.733286</td>\n",
              "      <td>0.627212</td>\n",
              "      <td>0.715213</td>\n",
              "      <td>0.586538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45dcb2dc-9735-4baa-976f-a23bb735a7ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45dcb2dc-9735-4baa-976f-a23bb735a7ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45dcb2dc-9735-4baa-976f-a23bb735a7ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2cf04502-0a8e-4079-93f9-cfe363fff6aa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2cf04502-0a8e-4079-93f9-cfe363fff6aa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2cf04502-0a8e-4079-93f9-cfe363fff6aa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "9Z4hZhGWQ4ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def test(model, prediction_dataloader):\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Tracking variables\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    # Predict\n",
        "    for batch in prediction_dataloader:\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_dt, b_labels = batch['dt'], batch['target']\n",
        "        b_dt = b_dt.to(device)\n",
        "        b_labels = b_labels.to(device)\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            logits = model(b_dt)\n",
        "\n",
        "        m, preds, labels = compute_score(METRICS['sentiment'], logits, b_labels, return_preds=True)\n",
        "        predictions.extend(preds)\n",
        "        true_labels.extend(labels)\n",
        "\n",
        "\n",
        "    score = pd.DataFrame([])\n",
        "    score['pred'] = predictions\n",
        "    score['true'] = true_labels\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "sshW2BZXaio7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "score = test(model, test_dataloader)\n",
        "\n",
        "y_true = score['true']\n",
        "y_pred = score['pred']\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "disp.plot()\n",
        "\n",
        "print(classification_report(y_pred=y_pred, y_true=y_true))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NOTwOYOUarDd",
        "outputId": "20cf6331-27fb-42b6-dc44-1c35ae4f1f08"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.12      0.21       189\n",
            "           1       0.55      0.96      0.70       211\n",
            "\n",
            "    accuracy                           0.56       400\n",
            "   macro avg       0.65      0.54      0.45       400\n",
            "weighted avg       0.64      0.56      0.47       400\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainations"
      ],
      "metadata": {
        "id": "PQHvOg_OQ89N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original notebook from Leonardo Ranaldi:\n",
        "https://github.com/LeonardRanaldi/Practical-NLP-2022"
      ],
      "metadata": {
        "id": "HaWRta-4zGwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[KERMITviz](https://www.researchgate.net/profile/Leondardo-Ranaldi/publication/359620068_KERMITviz_Visualizing_Neural_Network_Activations_on_Syntactic_Trees/links/625e660d1c096a380d0fa9e4/KERMITviz-Visualizing-Neural-Network-Activations-on-Syntactic-Trees.pdf)\n",
        "\n",
        "The KERMIT neural network has two main\n",
        "components: the KERMIT encoder, that encodes parse trees T in embedding vectors,\n",
        "and a multi-layer perceptron (MLP) that exploits these embedding vectors.\n",
        "\n",
        "$y = D(T) = W_{dt}x$\n",
        "\n",
        "$z = mlp(y)$\n",
        "\n",
        "The KERMIT encoder $D$ in Eq. 1 stems from tree kernels and distributed tree kernels.\n",
        "It gives the possibility to represent parse trees in vector spaces $R^d$ that embed huge spaces of subtrees $R^n$.\n",
        "These encoders may be seen as linear transformations $W_{dt} ‚àà R^{d√ón}$\n",
        "These linear transformations embed vectors $x^T ‚àà R^n$ in the\n",
        "space of tree kernels in smaller vectors $y\n",
        "^T ‚àà R^d$:\n",
        "\n",
        "\n",
        "$y^T = W_{dt}x^T$\n",
        "Columns $w_i$ of $W_{dt}$ encode subtree $œÑ\n",
        "(i)$\n",
        "\n",
        "\n",
        "\n",
        "LRP is a framework to explain the decisions of a generic neural network using local\n",
        "redistribution rules and is able to explain which input features contributed most to the\n",
        "final classification."
      ],
      "metadata": {
        "id": "hJj4Bl07zlCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kerMIT.tree_encode import parse as parse_tree\n",
        "from kerMIT.samples import utils\n",
        "\n",
        "from kerMIT import tree\n",
        "from kerMIT.dtk import DT\n",
        "from tqdm import tqdm\n",
        "import re, torch, pickle, copy\n",
        "from torchtext import data as datx\n",
        "from torch import nn, optim\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "VX09JpDd8XG5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feuAMz7sn7W-",
        "outputId": "e741d555-e688-4525-d690-ecbfca116685"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "#set manual seed for replicability\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(42)\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bC2lF9tnGzXA"
      },
      "outputs": [],
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "def getWeightAnBiasByName(layer_name):\n",
        "    weight, bias = _,_\n",
        "    for name, param in model.named_parameters():\n",
        "        if name == layer_name+'.weight' and param.requires_grad:\n",
        "            weight = param.data\n",
        "        elif name == layer_name+'.bias' and param.requires_grad:\n",
        "            bias = param.data\n",
        "    return weight, bias\n",
        "\n",
        "def lrp_linear_torch(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor=0.0, debug=False):\n",
        "    \"\"\"\n",
        "    LRP for a linear layer with input dim D and output dim M.\n",
        "    Args:\n",
        "    - hin:            forward pass input, of shape (D,)\n",
        "    - w:              connection weights, of shape (D, M)\n",
        "    - b:              biases, of shape (M,)\n",
        "    - hout:           forward pass output, of shape (M,) (unequal to np.dot(w.T,hin)+b if more than one incoming layer!)\n",
        "    - Rout:           relevance at layer output, of shape (M,)\n",
        "    - bias_nb_units:  total number of connected lower-layer units (onto which the bias/stabilizer contribution is redistributed for sanity check)\n",
        "    - eps:            stabilizer (small positive number)\n",
        "    - bias_factor:    set to 1.0 to check global relevance conservation, otherwise use 0.0 to ignore bias/stabilizer redistribution (recommended)\n",
        "    Returns:\n",
        "    - Rin:            relevance at layer input, of shape (D,)\n",
        "    \"\"\"\n",
        "    sign_out = torch.where(hout.cpu() >= 0 , torch.Tensor([1.]), torch.Tensor([-1.])).view(1,-1) # shape (1, M)\n",
        "\n",
        "    numer    = (w * hin.view(-1,1)).cpu() + ( bias_factor * (b.view(1,-1)*1. + eps*sign_out*1.) / bias_nb_units ) # shape (D, M)\n",
        "    # Note: here we multiply the bias_factor with both the bias b and the stabilizer eps since in fact\n",
        "    # using the term (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units in the numerator is only useful for sanity check\n",
        "    # (in the initial paper version we were using (bias_factor*b[na,:]*1. + eps*sign_out*1.) / bias_nb_units instead)\n",
        "\n",
        "    denom    = hout.view(1,-1) + (eps*sign_out*1.)   # shape (1, M)\n",
        "\n",
        "    message  = (numer/denom) * Rout.view(1,-1)       # shape (D, M)\n",
        "\n",
        "    Rin      = message.sum(axis=1)              # shape (D,)\n",
        "\n",
        "    if debug:\n",
        "        print(\"local diff: \", Rout.sum() - Rin.sum())\n",
        "    # Note:\n",
        "    # - local  layer   relevance conservation if bias_factor==1.0 and bias_nb_units==D (i.e. when only one incoming layer)\n",
        "    # - global network relevance conservation if bias_factor==1.0 and bias_nb_units set accordingly to the total number of lower-layer connections\n",
        "    # -> can be used for sanity check\n",
        "\n",
        "    return Rin\n",
        "\n",
        "def prepare_single_pass(activation, start_layer, end_layer, isFirstCompute=True):\n",
        "    hout = activation[start_layer].reshape(-1)\n",
        "    if end_layer != None:\n",
        "        hin = activation[end_layer].reshape(-1).cpu()\n",
        "    else:\n",
        "        hin = None\n",
        "\n",
        "    w, b = getWeightAnBiasByName(start_layer)\n",
        "    w = w.reshape(w.shape[1],w.shape[0])\n",
        "\n",
        "    bias_nb_units = b.shape[0]\n",
        "    eps = 0.001\n",
        "    bias_factor = 1.0\n",
        "\n",
        "    if isFirstCompute:\n",
        "        mask = torch.zeros( hout.shape[0] )\n",
        "        mask[ torch.argmax(hout) ] = hout[ torch.argmax(hout) ]\n",
        "        Rout = torch.Tensor(mask).cpu()\n",
        "    else:\n",
        "        Rout = None\n",
        "    return hin, w.cpu(), b.cpu(), hout.cpu(), Rout, bias_nb_units, eps, bias_factor\n",
        "\n",
        "def compute_LRP_FFNN(activation, layer_names, on_demand_embedding_matrix, single_test):\n",
        "    isFirstCompute = True\n",
        "    for i in range(len(layer_names)-1):\n",
        "        print(layer_names[i], layer_names[i+1])\n",
        "        hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor = prepare_single_pass(activation, layer_names[i], layer_names[i+1], isFirstCompute)\n",
        "        if not isFirstCompute:\n",
        "            Rout = Rin\n",
        "        Rin = lrp_linear_torch(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n",
        "        isFirstCompute = False\n",
        "    # compute the last layer\n",
        "    _, w, b, hout, _, bias_nb_units, eps, bias_factor = prepare_single_pass(activation, layer_names[-1], None, False)\n",
        "    Rout = Rin\n",
        "    hin = single_test.reshape(-1).cpu()\n",
        "\n",
        "    Rin = lrp_linear_torch(hin, torch.matmul(on_demand_embedding_matrix,w), b, hout, Rout, bias_nb_units, eps, bias_factor, debug=False)\n",
        "    return Rin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rbNY5EeEVFTc"
      },
      "outputs": [],
      "source": [
        "from kerMIT.explain.modelToExplain import lrp_DT,relevance_to_string\n",
        "from kerMIT.explain.activationSubtreeLRP import ActivationSubtreeLRP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pTda-xR7G_x3"
      },
      "outputs": [],
      "source": [
        "# resister forward hook for activation\n",
        "model.linear_1.register_forward_hook(get_activation('linear_1'))\n",
        "model.linear_2.register_forward_hook(get_activation('linear_2'))\n",
        "layer_names = ['linear_2','linear_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "lT3IjF9IWZRH",
        "outputId": "6f2f91d2-d28a-45be-db40-8e1d8a10c572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable))))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "INDEX = 40\n",
        "\n",
        "sentence = dataset['train']['sentence'][INDEX]\n",
        "target = dataset['train']['label'][INDEX]\n",
        "pt = dataset['train']['t'][INDEX]\n",
        "dt =  dataset['train']['dt'][INDEX]\n",
        "\n",
        "pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "QchSsRr4Hsy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36b3843-1211-42c8-a3a0-92989d835556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1141,  0.1360]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "#¬†prediction\n",
        "dt = torch.tensor(dt).unsqueeze(dim=0).to(device)\n",
        "print(dt.shape)\n",
        "output = model(dt)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(output, axis=1), target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqJlyYtuyrdd",
        "outputId": "2d5b542b-258d-4ad1-b385-a12cb86ebc4a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1], device='cuda:0'), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpt989SW_t7n"
      },
      "source": [
        "Given the tree t, this block computes its dtfs, the input vector of the subtrees, and the LRP activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "4kqorQi2bWOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7daa841d-107f-4683-9aea-ecd6d630a3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_2 linear_1\n",
            "[(, ,), array([0.86168563], dtype=float32)]\n",
            "[(ADJP (JJ acceptable)), array([0.7346217], dtype=float32)]\n",
            "[(JJ enjoyable), array([0.6341265], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) ADJP), array([0.3552652], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) ADJP)), array([0.32592922], dtype=float32)]\n",
            "[(ADJP RB RB JJ), array([0.1985154], dtype=float32)]\n",
            "[(ADJP RB (RB very) (JJ enjoyable)), array([0.18086755], dtype=float32)]\n",
            "[(ADJP RB RB (JJ enjoyable)), array([0.17684437], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable)))), array([0.13661127], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) ADJP), array([0.13306531], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP RB RB (JJ enjoyable))), array([0.12452768], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP RB RB (JJ enjoyable)))), array([0.12287387], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , ADJP)), array([0.11489128], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) RB (JJ enjoyable)))), array([0.10933615], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) RB JJ))), array([0.1083958], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP RB RB JJ)), array([0.10483965], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP RB (RB very) JJ))), array([0.10426935], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP RB (RB very) JJ))), array([0.10135269], dtype=float32)]\n",
            "[(ADJP (RB occasionally) (RB very) (JJ enjoyable)), array([0.09999841], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB (RB very) JJ)), array([0.09640478], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP RB (RB very) (JJ enjoyable))), array([0.09572968], dtype=float32)]\n",
            "[(ROOT FRAG), array([0.09504092], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP RB RB (JJ enjoyable))), array([0.09345787], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP RB RB JJ))), array([0.09085951], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) ADJP)), array([0.08326953], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) ADJP), array([0.08285371], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable)))), array([0.08211716], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable))), array([0.08204198], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP (RB occasionally) RB (JJ enjoyable))), array([0.0782645], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP (RB occasionally) RB JJ)), array([0.07501774], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP RB RB (JJ enjoyable)))), array([0.07438862], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) (RB very) JJ)), array([0.07432585], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP RB (RB very) JJ)), array([0.07154178], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP RB (RB very) JJ))), array([0.07143152], dtype=float32)]\n",
            "[(RB very), array([0.06718957], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) ADJP)), array([0.05819013], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP RB (RB very) (JJ enjoyable)))), array([0.05497803], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP RB RB JJ))), array([0.05479976], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB (RB very) (JJ enjoyable)))), array([0.05408631], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP (RB occasionally) (RB very) (JJ enjoyable)))), array([0.05279532], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) RB JJ))), array([0.05123032], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP (RB occasionally) RB (JJ enjoyable))), array([0.04811504], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) (RB very) JJ))), array([0.04671109], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB RB JJ))), array([0.04482891], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , ADJP)), array([0.04268703], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP (RB occasionally) (RB very) (JJ enjoyable))), array([0.04155801], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP (RB occasionally) RB (JJ enjoyable)))), array([0.03962941], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP RB RB (JJ enjoyable))), array([0.03527182], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP RB RB (JJ enjoyable)))), array([0.03302544], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , ADJP)), array([0.03224231], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) (RB very) JJ)), array([0.02979184], dtype=float32)]\n",
            "[(RB occasionally), array([0.02787463], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP RB RB (JJ enjoyable)))), array([0.02673164], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) (RB very) (JJ enjoyable))), array([0.02644351], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP RB (RB very) JJ)), array([0.02462144], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , ADJP), array([0.02363066], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP RB (RB very) (JJ enjoyable)))), array([0.02296925], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP (RB occasionally) (RB very) (JJ enjoyable))), array([0.02069298], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) RB (JJ enjoyable)))), array([0.02058609], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB RB JJ)), array([0.01991026], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP RB (RB very) (JJ enjoyable))), array([0.01972508], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP RB (RB very) (JJ enjoyable)))), array([0.0185679], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) RB (JJ enjoyable)))), array([0.01645047], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB (RB very) JJ))), array([0.01640437], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) RB (JJ enjoyable))), array([0.01635301], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP (RB occasionally) RB JJ))), array([0.01507696], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP RB (RB very) JJ)), array([0.01438618], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) (RB very) (JJ enjoyable)))), array([0.01396636], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP RB RB JJ))), array([0.01314532], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP RB RB (JJ enjoyable)))), array([0.01246135], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP RB RB JJ)), array([0.01001239], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable)))), array([0.00874409], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP RB RB JJ)), array([0.00874357], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP (RB occasionally) RB (JJ enjoyable)))), array([0.00849728], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) RB JJ)), array([0.00481191], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP (RB occasionally) (RB very) JJ)), array([0.00470276], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP (RB occasionally) (RB very) JJ))), array([0.0036233], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP RB (RB very) (JJ enjoyable)))), array([0.00319737], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP RB RB JJ))), array([0.00216435], dtype=float32)]\n",
            "[(ADJP (RB occasionally) RB (JJ enjoyable)), array([-0.00018348], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) (RB very) JJ))), array([-0.00197815], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP RB (RB very) JJ))), array([-0.00202366], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP (RB occasionally) RB JJ)), array([-0.00322409], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP (RB occasionally) RB (JJ enjoyable))), array([-0.00337567], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) RB JJ))), array([-0.00659013], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP (RB occasionally) RB JJ))), array([-0.01034277], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , ADJP), array([-0.01560523], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP (RB occasionally) RB JJ))), array([-0.01874237], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP RB (RB very) (JJ enjoyable))), array([-0.02189784], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP RB (RB very) JJ)), array([-0.02204163], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP (RB occasionally) (RB very) JJ))), array([-0.02205175], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) RB (JJ enjoyable))), array([-0.02471692], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP RB (RB very) (JJ enjoyable)))), array([-0.02731449], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) , (ADJP (RB occasionally) (RB very) (JJ enjoyable)))), array([-0.03004636], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) RB (JJ enjoyable))), array([-0.03297476], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) , (ADJP RB (RB very) JJ))), array([-0.03623831], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) (RB very) JJ)), array([-0.03625388], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB RB (JJ enjoyable))), array([-0.04186206], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP (RB occasionally) (RB very) JJ))), array([-0.04419615], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP (RB occasionally) RB JJ)), array([-0.04852083], dtype=float32)]\n",
            "[(ADJP (RB occasionally) (RB very) JJ), array([-0.04907663], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP (RB occasionally) RB JJ)), array([-0.04951855], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP RB RB JJ)), array([-0.05172198], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP RB RB JJ)), array([-0.0520451], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP (RB occasionally) (RB very) JJ)), array([-0.05487967], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP RB (RB very) (JJ enjoyable))), array([-0.05556915], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) (RB very) JJ))), array([-0.05823983], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP RB RB (JJ enjoyable))), array([-0.06049597], dtype=float32)]\n",
            "[(ROOT (FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB RB (JJ enjoyable)))), array([-0.07005056], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) , (ADJP (RB occasionally) RB JJ)), array([-0.0767385], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP , (ADJP RB RB JJ))), array([-0.07682183], dtype=float32)]\n",
            "[(FRAG (ADJP JJ) (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable))), array([-0.07860407], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP RB (RB very) JJ)), array([-0.07960722], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP (RB occasionally) (RB very) JJ)), array([-0.08611757], dtype=float32)]\n",
            "[(ROOT (FRAG ADJP (, ,) (ADJP (RB occasionally) RB (JJ enjoyable)))), array([-0.09445175], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) (, ,) (ADJP RB (RB very) (JJ enjoyable))), array([-0.09496705], dtype=float32)]\n",
            "[(FRAG ADJP (, ,) (ADJP (RB occasionally) (RB very) (JJ enjoyable))), array([-0.11014614], dtype=float32)]\n",
            "[(FRAG (ADJP (JJ acceptable)) , (ADJP RB RB (JJ enjoyable))), array([-0.1307083], dtype=float32)]\n",
            "[(FRAG ADJP , (ADJP RB (RB very) (JJ enjoyable))), array([-0.14124852], dtype=float32)]\n",
            "[(ADJP (RB occasionally) RB JJ), array([-0.19375893], dtype=float32)]\n",
            "[(JJ acceptable), array([-0.27364957], dtype=float32)]\n",
            "[(FRAG ADJP , ADJP), array([-0.2763918], dtype=float32)]\n",
            "[(ADJP RB (RB very) JJ), array([-0.33360556], dtype=float32)]\n",
            "[(ADJP JJ), array([-0.5356702], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "tree_pt = tree.Tree(string=pt)\n",
        "act_lrp = ActivationSubtreeLRP(encoder)\n",
        "\n",
        "_, tree_index_dict, on_demand_embedding_matrix, tree_index_dict_inverse, activation_vector = act_lrp.on_demand_embedding_matrix(tree_pt)\n",
        "\n",
        "on_demand_embedding_matrix = torch.Tensor(on_demand_embedding_matrix)\n",
        "activation_vector = torch.Tensor(activation_vector)\n",
        "relevance = compute_LRP_FFNN(activation, layer_names, on_demand_embedding_matrix, activation_vector)\n",
        "\n",
        "act_tree_lrp = act_lrp.saveActivation(tree_pt, relevance, tree_index_dict)\n",
        "active_subtrees = sorted(act_tree_lrp['act_sub_trees'], key=lambda x:x[1], reverse=True)\n",
        "for t in active_subtrees:\n",
        "  print(t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSCE00Edhvp-",
        "outputId": "56b00b9e-90ef-4ac6-827d-f96813944ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 142.06it/s]\n"
          ]
        }
      ],
      "source": [
        "from kerMIT.explain import kerMITviz\n",
        "\n",
        "#visualization of previously extracted activations through heat parse tree\n",
        "heat_parse_tree = kerMITviz.assign_contribution_nodes(act_tree_lrp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MWk_52NsTFt",
        "outputId": "888d0c3c-06d6-4fe9-a354-46b72471c6ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'ACT_0': '(ROOT::0.26110142 (FRAG::0.3015607 (ADJP::0.8247574 (JJ::0.8247574 acceptable:0.8247574)) (,::1.0 ,:1.0) (ADJP::0.3015607 (RB::0.085376255 occasionally:0.0) (RB::0.085376255 very:0.061036892) (JJ::0.6861575 enjoyable:0.6861575))))'}]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "heat_parse_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "IAEUIKQFrCvp"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import fileinput"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display heat parse tree"
      ],
      "metadata": {
        "id": "onpTP9YQzhaY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "tJAfHZvHrh_W"
      },
      "outputs": [],
      "source": [
        "brotherDistance = \"5\"\n",
        "levelLength = \"50\"\n",
        "#brotherDistance = input('Enter brother Distance in px (recommended 5): ')\n",
        "#levelLength = input('Enter level Lenght in px (recommended 50: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "o0uj5iD_rCyn"
      },
      "outputs": [],
      "source": [
        "string = str(heat_parse_tree)\n",
        "\n",
        "name = \"var string\"\n",
        "name1 = \"var brotherDistance\"\n",
        "name2 = \"var levelLength\"\n",
        "\n",
        "!cp '/content/KERMIT/Visualizer/notebook/index.html' '/content/KERMIT/Visualizer/notebook/index_New.html'\n",
        "!cp '/content/KERMIT/Visualizer/notebook/index2.html' '/content/KERMIT/Visualizer/notebook/index2_New.html'\n",
        "\n",
        "\n",
        "with open('/content/KERMIT/Visualizer/notebook/index_New.html','r') as names_file:\n",
        "    content = names_file.read()\n",
        "    new = content.replace(name, '='.join([name, '\"' + string + '\"']))\n",
        "    new = new.replace(name1, '='.join([name1, brotherDistance]))\n",
        "    new = new.replace(name2, '='.join([name2, levelLength]))\n",
        "\n",
        "with open('/content/KERMIT/Visualizer/notebook/index_New.html','w') as new_names_file:\n",
        "    new_names_file.write(new)\n",
        "\n",
        "\n",
        "with open('/content/KERMIT/Visualizer/notebook/index2_New.html','r') as names_file:\n",
        "    content = names_file.read()\n",
        "    new = content.replace(name, '='.join([name, '\"' + string + '\"']))\n",
        "    new = new.replace(name1, '='.join([name1, brotherDistance]))\n",
        "    new = new.replace(name2, '='.join([name2, levelLength]))\n",
        "\n",
        "with open('/content/KERMIT/Visualizer/notebook/index2_New.html','w') as new_names_file:\n",
        "    new_names_file.write(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "z9vszdJtrC4S",
        "outputId": "56c473e7-0824-4f96-e631-44e17aa17bb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
              "    <title>KERMIT-viz</title>\n",
              "\n",
              "    <style>\n",
              "        * {\n",
              "            box-sizing: border-box;\n",
              "        }\n",
              "\n",
              "\n",
              "        input[type=text],\n",
              "        select,\n",
              "        textarea {\n",
              "            width: 100%;\n",
              "            padding: 12px;\n",
              "            border: 1px solid #ccc;\n",
              "            border-radius: 4px;\n",
              "            resize: vertical;\n",
              "        }\n",
              "\n",
              "        label {\n",
              "            padding: 12px 12px 12px 0;\n",
              "            display: inline-block;\n",
              "        }\n",
              "\n",
              "        input[type=submit] {\n",
              "            background-color: #4CAF50;\n",
              "            color: white;\n",
              "            padding: 12px 20px;\n",
              "            border: none;\n",
              "            border-radius: 4px;\n",
              "            cursor: pointer;\n",
              "            float: right;\n",
              "        }\n",
              "\n",
              "        input[type=number] {\n",
              "            width: 25%;\n",
              "            margin-top: 10px;\n",
              "            border-radius: 4px;\n",
              "            border: 1px solid #ccc;\n",
              "            padding: 12px;\n",
              "            resize: vertical;\n",
              "        }\n",
              "\n",
              "        input[type=submit]:hover {\n",
              "            background-color: #45a049;\n",
              "        }\n",
              "\n",
              "        .container {\n",
              "            border-radius: 5px;\n",
              "            padding: 20px;\n",
              "            background-color: darkseagreen;\n",
              "        }\n",
              "\n",
              "        .col-25 {\n",
              "            float: left;\n",
              "            width: 25%;\n",
              "            margin-top: 6px;\n",
              "            text-align: center;\n",
              "        }\n",
              "\n",
              "        .col-75 {\n",
              "            width: 75%;\n",
              "            margin-top: 6px;\n",
              "            float: left;\n",
              "        }\n",
              "\n",
              "        /* Clear floats after the columns */\n",
              "        .row:after {\n",
              "            content: \"\";\n",
              "            display: table;\n",
              "            clear: both;\n",
              "        }\n",
              "\n",
              "\n",
              "        /* Responsive layout - when the screen is less than 600px wide, make the two columns stack on top of each other instead of next to each other */\n",
              "        @media screen and (max-width: 600px) {\n",
              "\n",
              "            .col-25,\n",
              "            .col-75,\n",
              "            input[type=submit] {\n",
              "                width: 100%;\n",
              "                margin-top: 0;\n",
              "            }\n",
              "\n",
              "            p {\n",
              "                text-align: center;\n",
              "            }\n",
              "        }\n",
              "\n",
              "        h1 {\n",
              "            text-align: center;\n",
              "        }\n",
              "\n",
              "        button {\n",
              "            background-color: #4CAF50;\n",
              "            color: white;\n",
              "            padding: 12px 20px;\n",
              "            border: none;\n",
              "            border-radius: 4px;\n",
              "            cursor: pointer;\n",
              "        }\n",
              "\n",
              "        button:hover {\n",
              "            background-color: #45a049;\n",
              "        }\n",
              "\n",
              "\n",
              "        p {\n",
              "            clear: both;\n",
              "        }\n",
              "\n",
              "        ul {\n",
              "            padding: 15px;\n",
              "            margin: 10px;\n",
              "        }\n",
              "\n",
              "        ul li {\n",
              "            margin-top: 5px;\n",
              "        }\n",
              "\n",
              "        footer {\n",
              "            margin-top: 40px;\n",
              "        }\n",
              "    </style>\n",
              "\n",
              "</head>\n",
              "\n",
              "<body>\n",
              "    <h1>Smart visualization of Heat Parse Trees for KERMIT</h1>\n",
              "\n",
              "    <div>\n",
              "        <canvas id=\"myCanvas\" width=\"100%\" height=\"100%\" style=\"border:1px solid #000000;\">\n",
              "        </canvas>\n",
              "    </div>\n",
              "    <p>\n",
              "        <button type=\"button\" id=\"myBtn\" onclick=\"downloadCanvas()\">Download Image</button>\n",
              "    </p>\n",
              "    <footer>\n",
              "         ¬© ART Group, Universit√† degli Studi di Roma \"Tor Vergata\", code avaiable on <a href=\"https://github.com/ART-Group-it/KERMIT\" target=\"blank\"> <i\n",
              "                class=\"fa fa-github\" aria-hidden=\"true\"></i></a>\n",
              "    </footer>\n",
              "</body>\n",
              "\n",
              "</html>\n",
              "\n",
              "\n",
              "\n",
              "<script>\n",
              "var string=\"[{'ACT_0': '(ROOT::0.26110142 (FRAG::0.3015607 (ADJP::0.8247574 (JJ::0.8247574 acceptable:0.8247574)) (,::1.0 ,:1.0) (ADJP::0.3015607 (RB::0.085376255 occasionally:0.0) (RB::0.085376255 very:0.061036892) (JJ::0.6861575 enjoyable:0.6861575))))'}]\"\n",
              "var brotherDistance=5\n",
              "var levelLength=50\n",
              "\n",
              "var result = [];\n",
              "\n",
              "var res = string.slice(string.indexOf('('), string.lastIndexOf(\"'\"));\n",
              "var pilaParent = [];\n",
              "var firstIndex = res.indexOf(':');\n",
              "var j = 0;\n",
              "for (i = 0; i <= res.length; i++) {\n",
              "\n",
              "    if (res[i] == '(') {\n",
              "        var nome = computeName(i);\n",
              "        i = i + nome.length;\n",
              "        var valore = computeValue(i + 1);\n",
              "        i = i + valore.length;\n",
              "\n",
              "        if (j == 0) {\n",
              "            result[j] = {\n",
              "                name: nome,\n",
              "                value: valore,\n",
              "                parent: {},\n",
              "                id: j,\n",
              "                disegnato: false\n",
              "            }\n",
              "            pilaParent.push(result[j]);\n",
              "        } else {\n",
              "            result[j] = {\n",
              "                name: nome,\n",
              "                value: valore,\n",
              "                parent: {\n",
              "                    padre: pilaParent[pilaParent.length - 1]\n",
              "                },\n",
              "                id: j,\n",
              "                disegnato: false\n",
              "            }\n",
              "            pilaParent.push(result[j]);\n",
              "        }\n",
              "        j++;\n",
              "    }\n",
              "\n",
              "\n",
              "    if (res[i] == ' ' && res[i + 1] != '(') {\n",
              "        var nome = computeName(i);\n",
              "        i = i + nome.length;\n",
              "        var valore = computeValue(i + 1);\n",
              "        i = i + valore.length;\n",
              "\n",
              "        result[j] = {\n",
              "            name: nome,\n",
              "            value: valore,\n",
              "            parent: {\n",
              "                padre: pilaParent[pilaParent.length - 1]\n",
              "            },\n",
              "            id: j,\n",
              "            disegnato: false\n",
              "        }\n",
              "        j++;\n",
              "    }\n",
              "\n",
              "    if (res[i] == ')') {\n",
              "        pilaParent.pop();\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "for (i = 0; i < result.length; i++) {\n",
              "    var nodo = result[i].id;\n",
              "    var figli = [];\n",
              "    for (j = i; j < result.length; j++) {\n",
              "        if (j == 0) {\n",
              "            j++;\n",
              "        }\n",
              "        if (nodo == result[j].parent.padre.id) {\n",
              "            figli.push(result[j]);\n",
              "        }\n",
              "    }\n",
              "    result[i].children = figli;\n",
              "}\n",
              "\n",
              "function computeName(index) {\n",
              "    var primaDuePunti = res.indexOf(\":\", index)\n",
              "\n",
              "    if (res[primaDuePunti + 1] == res[primaDuePunti]) {\n",
              "        return res.slice(index + 1, primaDuePunti + 1);\n",
              "    } else {\n",
              "        return res.slice(index + 1, primaDuePunti);\n",
              "    }\n",
              "}\n",
              "\n",
              "function computeValue(index) {\n",
              "    var inizioNumeri = index + 1;\n",
              "    var number = '';\n",
              "    while ((res[inizioNumeri] != ' ') && (res[inizioNumeri] != ')') && (inizioNumeri < res.length)) {\n",
              "        number = number + res[inizioNumeri];\n",
              "        inizioNumeri++;\n",
              "    }\n",
              "    if (isNaN(number)) {\n",
              "        var ultimiDuePunti = number.lastIndexOf(\":\") + 1;\n",
              "        number = number.slice(ultimiDuePunti);\n",
              "    }\n",
              "    return number;\n",
              "}\n",
              "\n",
              "\n",
              "var livello = 0;\n",
              "\n",
              "function calcolaAltezza(dataStructure, level) {\n",
              "\n",
              "    var numChild = dataStructure.children.length;\n",
              "    dataStructure.livello = level;\n",
              "    if (numChild == 0) {\n",
              "        return;\n",
              "    }\n",
              "    for (let i = 0; i < dataStructure.children.length; i++) {\n",
              "        calcolaAltezza(dataStructure.children[i], level + 1)\n",
              "    }\n",
              "    level = level + 1;\n",
              "    if (level > livello) {\n",
              "        livello = level;\n",
              "    }\n",
              "}\n",
              "calcolaAltezza(result[0], 1)\n",
              "\n",
              "\n",
              "var canvas = document.getElementById('myCanvas');\n",
              "canvas.height = levelLength * (livello + 1)\n",
              "canvas.width = window.innerWidth;\n",
              "\n",
              "\n",
              "var spazioOccupato = 0;\n",
              "\n",
              "function drawTree(dataStructure, level) {\n",
              "    var daDisegnare = true;\n",
              "    var numChild = dataStructure.children.length;\n",
              "\n",
              "    for (let i = 0; i < numChild; i++) {\n",
              "        drawTree(dataStructure.children[i], level + 1);\n",
              "    }\n",
              "\n",
              "    if (numChild == 0) { //base case\n",
              "        var spazioNodo = dataStructure.name.length * (4.3 + (parseFloat(dataStructure.value) * 10))\n",
              "        drawNode(dataStructure, spazioOccupato, level * levelLength);\n",
              "        spazioOccupato += spazioNodo + brotherDistance;\n",
              "        daDisegnare = false;\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < dataStructure.children.length; i++) {\n",
              "        if (dataStructure.children[i].disegnato == false) {\n",
              "            daDisegnare = false;\n",
              "        }\n",
              "    }\n",
              "\n",
              "    if (daDisegnare == true) {\n",
              "        if (dataStructure.children.length == 1) {\n",
              "            var spazioNodo = dataStructure.name.length * (4.3 + (parseFloat(dataStructure.value) * 10))\n",
              "            var spazioNodoFiglio = dataStructure.children[0].name.length * (4.3 + (parseFloat(dataStructure.value) * 10))\n",
              "            drawNode(dataStructure, dataStructure.children[0].x, level * levelLength)\n",
              "            if (spazioOccupato < (spazioOccupato - spazioNodoFiglio + spazioNodo)) {\n",
              "                spazioOccupato = (spazioOccupato - spazioNodoFiglio + spazioNodo);\n",
              "            }\n",
              "        } else {\n",
              "            var XprimoFiglio = dataStructure.children[0].x;\n",
              "            var XultimoFiglio = dataStructure.children[dataStructure.children.length - 1].x;\n",
              "\n",
              "            drawNode(dataStructure, (XultimoFiglio - XprimoFiglio) / 2 + XprimoFiglio, level * levelLength);\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "drawTree(result[0], 1);\n",
              "drawBranch(result[0], 1);\n",
              "canvas.width = spazioOccupato;\n",
              "spazioOccupato = 0;\n",
              "drawTree(result[0], 1);\n",
              "drawBranch(result[0], 1);\n",
              "\n",
              "\n",
              "\n",
              "function drawNode(node, center, height) {\n",
              "    node.x = center;\n",
              "    node.y = height;\n",
              "    node.disegnato = true;\n",
              "    var ctx = canvas.getContext(\"2d\");\n",
              "    ctx.font = 10 + Math.round(node.value * 10) + \"px\" + \"  monospace\";\n",
              "    ctx.fillStyle = perc2color(node.value);\n",
              "    ctx.imageSmoothingEnabled = false;\n",
              "    ctx.fillText(node.name, center, height);\n",
              "}\n",
              "\n",
              "function drawBranch(node, level) {\n",
              "\n",
              "    var numChild = node.children.length;\n",
              "\n",
              "    for (let i = 0; i < numChild; i++) {\n",
              "        drawBranch(node.children[i], level + 1);\n",
              "        var ctx = canvas.getContext(\"2d\");\n",
              "        ctx.strokeStyle = perc2color(node.children[i].value);\n",
              "        ctx.imageSmoothingEnabled = false;\n",
              "        ctx.beginPath();\n",
              "        if (node.children[i].children.length == 0) {\n",
              "            ctx.moveTo(node.x + 4.3, node.y + 3);\n",
              "            ctx.lineTo(node.x + 4.3, node.children[i].y - 15);\n",
              "        } else {\n",
              "            var spazioNodo = node.children[i].name.length * (4.3 + (parseFloat(node.children[i].value) * 10))\n",
              "            var spazioNodoPadre = node.name.length * (4.3 + (parseFloat(node.value) * 10))\n",
              "            ctx.moveTo(node.x + spazioNodoPadre / 2, node.y + 3);\n",
              "            ctx.lineTo(node.children[i].x + spazioNodo / 2, node.children[i].y - 15)\n",
              "        }\n",
              "        ctx.closePath();\n",
              "        ctx.stroke();\n",
              "    }\n",
              "}\n",
              "\n",
              "function isEmpty(obj) {\n",
              "    return Object.keys(obj).length === 0;\n",
              "}\n",
              "\n",
              "function perc2color(perc) {\n",
              "    perc = perc * 100;\n",
              "    var r, g, b = 0;\n",
              "    if (perc < 50) {\n",
              "        r = Math.round(5.1 * perc);\n",
              "        g = Math.round(5.1 * perc);\n",
              "        b = Math.round(3.1 * perc);\n",
              "    } else {\n",
              "        g = Math.round(510 - 5.1 * perc);\n",
              "        r = 255;\n",
              "    }\n",
              "    var h = r * 0x10000 + g * 0x100 + b * 0x1;\n",
              "    return '#' + ('000000' + h.toString(16)).slice(-6);\n",
              "}\n",
              "\n",
              "function downloadCanvas() { //function that download the Canvas in png\n",
              "    var link = document.createElement('a');\n",
              "    link.download = \"canvas.png\";\n",
              "    link.href = document.getElementById(\"myCanvas\").toDataURL(\"image/png\");\n",
              "    link.click();\n",
              "}\n",
              "\n",
              "</script>\n"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "IPython.display.HTML(filename='/content/KERMIT/Visualizer/notebook/index2_New.html')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4f430d0e55b4b2286a4adef10b8b4df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_512828c74c9d4c11aee92ca55ccf9d79",
              "IPY_MODEL_5fb12a5ab1dd4c6d809bc085207e3ce1",
              "IPY_MODEL_9237036b37e3430b8df863867e3ee6ff"
            ],
            "layout": "IPY_MODEL_28ab29fea45c491692261f2b04f9aa1c"
          }
        },
        "512828c74c9d4c11aee92ca55ccf9d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d171848d0a2a4550ad1cb8198098c62a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_55225a11da7345df872c94a5a1184678",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: "
          }
        },
        "5fb12a5ab1dd4c6d809bc085207e3ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_936ae44243764e6dbd842e4776910f8d",
            "max": 46218,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7becc734f25441f8057437201de242b",
            "value": 46218
          }
        },
        "9237036b37e3430b8df863867e3ee6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f6aa4b3899149f3ac09dd8e3632b183",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5b395d6139164f3ca1215bb9aa28cb0e",
            "value": " 370k/? [00:00&lt;00:00, 1.09MB/s]"
          }
        },
        "28ab29fea45c491692261f2b04f9aa1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d171848d0a2a4550ad1cb8198098c62a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55225a11da7345df872c94a5a1184678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "936ae44243764e6dbd842e4776910f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7becc734f25441f8057437201de242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f6aa4b3899149f3ac09dd8e3632b183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b395d6139164f3ca1215bb9aa28cb0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}